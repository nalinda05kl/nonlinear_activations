### Analysis Code for the Stydy: Effects of Nonlinearity and Network Architecture on the Performance of Supervised Neural Networks. 
Empirical study of the effects of nonlinearity in activation functions on the performance of deep learning models. We investigate the learning dynamics of hidden layers of shallow neural networks using entropy as a measurment of randomness in hidden layer outputs.

### This repository consists of following codes:
 - **File: performance_vs_alpha.py**: The accuracy and loss calculated for different model architecturs using the MNIST data set.
 - **Folder: performance_vs_data_domain**: Loss calculated based on different data domains.
 - **Folder: Entropy**: Entropy calculation for each layer and its variations under different nonlinearities and model architectures
 
 ### Data used for the analysis:
 - **MNIST hand written digits**: http://yann.lecun.com/exdb/mnist/
 - **FOOD-11 data set**: https://www.kaggle.com/vermaavi/food11
